# Actividad 2 - Optimizador y Cross Entrompy

1.  Implementar sobre su codigo de red neuronal de la tarea anterior, algun optimizador visto en clase (RMS, BackProp con inercia, Adam).
2.  Hacer captura de pantalla resultante de haber corrido el codigo.
3.  Hacer commit con descripcion de lo hecho y si mejoró o no el porcentajede aciertos. ¿Qué porcentaje?.
4.  Empujar  (push)  su  codigo  a  github.   (verificar  que  los  archivos  en  git-hub.com contengan los cambios y la captura de pantalla)
5.  Implementar cross-entropy con capa Soft-Max.  Hacer captura del resultado.   Hacer  commit  con  descripcion  de  lo  hecho  y  si  mejoró  o  no  elporcentaje de aciertos.  ¿Qué porcentaje?
6.  Empujar (push) su c ́odigo a github.

---

En el siguiente [link](https://github.com/Jeremy-22/RN/blob/main/Optimizador_y_Cross-Entropy/PRN1.py)
se puede visualizar el codigo con la implementación del otimizador BackProp con inercia
<p align="center">
  <img src="Evidencia.png" width="300" height="500" />
</p>
Se puede apreciar se comienza con una eficiencia de 90.16% y termina con una eficiencia de 94.25%, esto con respecto a la red sin implementar el optimizador se puede apreciar que comienza con mejor eficiencia pero termina con una menor.
