{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):       # clase del objeto con argumneto su herencia, es decir,\n",
    "                          #como primer padre una misma clase: la clase Object.\n",
    "\n",
    "    def __init__(self, sizes):#se define la función init (como atributo privado) con primer argumento \"self\" \n",
    "                           #(Es una variable especial de sólo lectura que proporciona Python), y\n",
    "                           #como segundo argumento algún parametro o objeto, en nuestro caso sizes\n",
    "        self.num_layers = len(sizes) # se toma al objeto num_layer(número de capas) dentro del objeto self\n",
    "                                     # y se le asigna devolver el número de objetos dentro del objeto sizes\n",
    "        self.sizes = sizes           #atributo publico\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]] #list comprehension, esta iterando sobre la \n",
    "                               #lista de capas np.random.randn(y, 1) genera matrices con entradas aleatorias\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.epsilon =0.00001\n",
    "        self.g2 = 0.2\n",
    "        self.beta = 0.9\n",
    "        #Los pesos (w) son una matriz que relaciona las flechas entre una capa y otra\n",
    "        #Esto es para entrenar la red neuronal y poderla usar lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " def feedforward(self, a): #Se define feedforward, para evaluar la red neuronal\n",
    "                           #(self, a(activaciones de la primera capa))\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):   # zip pega los vectores elemento a elemento\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "#Aquí se definio el Stochastic Gradient Descent, con trainig data como la lista de Tuplas \"(x,y)\"\n",
    "\n",
    "\n",
    "     #   \"\"\"Train the neural network using mini-batch stochastic\n",
    "      #  gradient descent.  The \"training_data\" is a list of tuples\n",
    "       # \"(x, y)\" representing the training inputs and the desired\n",
    "        #outputs.  The other non-optional parameters are\n",
    "        #self-explanatory.  If \"test_data\" is provided then the\n",
    "        #network will be evaluated against the test data after each\n",
    "        #epoch, and partial progress printed out.  This is useful for\n",
    "        #tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data) #trasformar los datos de entrenamiento en una lista, o tupla\n",
    "        n = len(training_data)  #El número de datos tenemos para entrenar\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data) #numero de datos de prueba\n",
    "\n",
    "        for j in xrange(epochs): #Es para iterar sobre el numero de epocas que se deasea entrenar\n",
    "            random.shuffle(training_data) # Es como barajar los elementos de la lista \n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]# el tamaño de los mini_batches que se utilixzara \n",
    "                                            # para hacer el muestreo\n",
    "                for k in xrange(0, n, mini_batch_size)] # donde eta es la taza de aprendizaje\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test))\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "            #Si se proporciona el argumento opcional test_data, \n",
    "            #entonces el programa evaluará la red después de cada época de entrenamiento e \n",
    "            #imprimirá el progreso parcial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update_mini_batch(self, mini_batch, eta): #para cada mini_batch se aplica un único paso de descenso \n",
    "                                             #del gradiente.\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]#una vez calculados los gradientes se guardan\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch: # Aquí x es la entrada de la red, y y es el dato real que queremos que nos de\n",
    "            #ademas de actualizar los pesos y sesgos de la red de acuerdo con una única iteración del descenso\n",
    "            # de gradiente, utilizando solo los datos de entrenamiento en mini_batch.\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y) # esto nos da el algoritmo de retropropagación\n",
    "            #lo que es una forma más eficiente de calcular la funcion de costo\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "            #Se comienza mezclando aleatoriamente los datos de entrenamiento y luego los divide en \n",
    "            #minilotes del tamaño apropiado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(self, x, y): #esto nos devuelveuna tupla que representa el\n",
    "         #gradiente para la función de costo C_x\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]#son listas de capa por capa o elemento a\n",
    "                                            #elemento de matrices similiraes\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # genera una lista que alamacena todas las activaciones capa por capa\n",
    "        zs = [] # hace lo mismo que arriba pero para z \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b  #argumento de la sigmoide. np.dot(w, activation)=wx\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        for l in range(2, self.num_layers): # l variable l es pequeña y l=1 es la ultima capa de la red\n",
    "            z = zs[-l]  # y -1 la penultima y así sucesivamente, z respresenta la entreda de la capa actual\n",
    "            sp = sigmoid_prime(z) #es la derivada de la función de activación aplicada a z\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp #epresenta el error en la capa actual,\n",
    "                                   # y se calcula utilizando los pesos y el error en la capa siguiente\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) #representan los gradientes de los \n",
    "                                      #sesgos y pesos\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "def evaluate(self, test_data):#numero de datos que acerto la red\n",
    "        \n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data] #Los datos de prueba son una lista de tuplas\n",
    "        return sum(int(x == y) for (x, y) in test_results)# devuelve l número de entradas de prueba \n",
    "                                            #para las cuales la red neuronal genera el resultado correcto.\n",
    "\n",
    "def cost_derivative(self, output_activations, y): #derivada de la funcion de costo\n",
    "        \n",
    "        return (output_activations-y) #esto nos devuelve el vector de derivadas parciales de la funcion\n",
    "        #de costo \n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z)) #nos regresa la funcion sigmoidal de la capa  \n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z)) #nos regresa la derivada de la funcion sigmoidal de la capa  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
